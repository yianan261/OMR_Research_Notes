{
	"nodes":[
		{"id":"196af9ae129b7f17","type":"group","x":520,"y":-1520,"width":2480,"height":1600,"color":"4","label":"OMR Techniques"},
		{"id":"2a5f57fc75eda644","type":"group","x":-300,"y":240,"width":780,"height":800,"color":"2","label":"Evaluation"},
		{"id":"da5f478fc98c81c4","type":"group","x":640,"y":360,"width":915,"height":680,"color":"#aa22ee","label":"Datasets"},
		{"id":"d41cf02ea0d396c1","type":"group","x":-120,"y":-2190,"width":875,"height":540,"color":"6","label":"Overview of OMR"},
		{"id":"e4ca83bfd9458a0e","type":"group","x":920,"y":-2620,"width":585,"height":700,"color":"#ff6666","label":"Project Goals"},
		{"id":"d6f5fce3261ed6a4","type":"group","x":2440,"y":-440,"width":417,"height":393,"label":"Eval Metrics"},
		{"id":"ddb4bba370f8dbf2","type":"text","text":"Evalution","x":-40,"y":280,"width":250,"height":60,"color":"#d54407"},
		{"id":"167c0aef7b0a29fc","type":"file","file":"papers/evaluation papers/A Case for Intrinsic Evaluation of Optical Music Recognition_compressed.pdf","x":-240,"y":440,"width":400,"height":400,"color":"#d54407"},
		{"id":"1397383c3bee83ea","type":"text","text":" Notes[[Understanding OMR]]\nPaper \n> [!PDF|255, 208, 0] [[understandingOMR.pdf#page=1&annotation=977R|understandingOMR, p.1]]\n> > Jorge Calvo-Zaragoza*,  University of Alicante Jan HajiË‡c Jr.*,  Charles University Alexander Pacha*,  TU Wien\n> \n> ","x":-100,"y":-1950,"width":340,"height":200,"color":"6"},
		{"id":"8caf05d5f62eb707","type":"text","text":"OMR overview","x":100,"y":-2170,"width":250,"height":60,"color":"6"},
		{"id":"7395ae67dcd40201","type":"text","text":"1. Improve upon current OMR systems/algorithms and generate MIDI files or other formats from printed music sheets/scores\n2. Generate audio files from the MIDI files to synthesis software to produce audio file\n3. Transcribing OMR -> developing pipeline of converting sheet music into machine-readable format. Process includes extraction, interpretation, and encoding musical information from printed symbols on the page and produce digital representation that preserves music's content and structure. Final goal is to generate audio format of that digitized representation. Should combine both combine **agnostic** and **semantic** representations to balance the strengths of each approach.\n[[Project Goals]]","x":940,"y":-2560,"width":492,"height":520,"color":"#ff6666"},
		{"id":"598ed405a08870ce","type":"text","text":"Optical Music Recognition-Recent Advances, Current Challenges, Future Directions Notes\n[[Optical Music Recognition-Recent Advances, Current Challenges, Future Directions Notes]]\n[[OMR_Advances_Challenges_diresctions_2023_compressed.pdf]]","x":280,"y":-1975,"width":325,"height":250,"color":"6"},
		{"id":"edd25bfc0795618e","type":"text","text":"Transformers","x":750,"y":-1442,"width":250,"height":60,"color":"2"},
		{"id":"5eca21d495ec50d5","type":"text","text":"Notes: Sheet Music Transformer (E2E)\n**Transformer-based image-to-sequence approach**\nPaper: \n![[Sheet Music Transformer- End-To-End Optical Music Recognition Beyond Monophonic Transcription-compressed 1.webp]]\n\n[[Sheet Music Transformer- End-To-End Optical Music Recognition Beyond Monophonic Transcription-compressed.pdf#page=1&rect=131,518,497,683|Sheet Music Transformer- End-To-End Optical Music Recognition Beyond Monophonic Transcription-compressed, p.1]]\nE2E: no multi-stage pipeline\nprocessing the entire image as a single input, model learns global context \nencoder: extracts feature maps from input image using CNN or transfomer-based backbones\ndecoder: autoregressive transformer to predict sequence symbol by symbol\n2D positional encoding to ensure spatial relationships in image\n[[Sheet Music Transformer Notes]]","x":565,"y":-1240,"width":493,"height":520,"color":"2"},
		{"id":"83da196e994781c7","type":"text","text":"https://github.com/apacha/MusicSymbolClassifier","x":1055,"y":750,"width":360,"height":90,"color":"#aa2bee"},
		{"id":"92c216a920bd4678","type":"text","text":"https://apacha.github.io/OMR-Datasets/","x":1175,"y":540,"width":360,"height":120,"color":"#aa2bee"},
		{"id":"b2f8988ba798196c","type":"text","text":"List of  datasets (includes handwritten and printed sources)\n> [!PDF|] [[Towards_a_Universal_Music_Symbol_Classifier.pdf#page=1&selection=0,42,0,42|Towards_a_Universal_Music_Symbol_Classifier, p.1]]\n> > Towards a Universal Music Symbol Classifier\n> \n> ","x":660,"y":600,"width":355,"height":360,"color":"#aa2bee"},
		{"id":"aca33838b0d96eed","type":"text","text":"Datatsets","x":890,"y":380,"width":250,"height":80,"color":"#aa2bee"},
		{"id":"c498cbb0891fa771","type":"text","text":"Methods","x":1248,"y":-630,"width":250,"height":50,"color":"3"},
		{"id":"6945e37f4f65e645","type":"text","text":"LLM","x":1795,"y":-665,"width":250,"height":60,"color":"#ff8f33"},
		{"id":"b6e51ec2374d1631","type":"text","text":"Main steps for OMR pipelines","x":1826,"y":-1472,"width":250,"height":60,"color":"2"},
		{"id":"c10fa58037cf7a00","type":"text","text":"TItle: OMR: state of the art and open issues (cited by 400+)\nDiscusses some common processing techniques and OMR algorithms, as well as evaluation issues\n\nNotes:\n![[state-of-the-art_and_issues.pdf#page=1&selection=9,0,29,15&color=yellow]]\n","x":1058,"y":-450,"width":315,"height":395,"color":"3"},
		{"id":"179bb11ce9577fdd","type":"text","text":"How to segment complex, dense scores with CNN\n[[Bootstrapping_Samples_of_Accidentals_in_Dense_Piano_Scores_for_CNN-Based_Detection.pdf#page=1&selection=0,45,1,36|Bootstrapping_Samples_of_Accidentals_in_Dense_Piano_Scores_for_CNN-Based_Detection, p.1]]","x":1403,"y":-435,"width":250,"height":335,"color":"3"},
		{"id":"b5ab0945e35ae7cf","type":"text","text":"Paligemma \n\n[[Paligemma2 Notes]]\ngeneral-purpose vision language model. \nCombines a **SigLIP-So400m vision encoder** with **Gemma 2 language models**\n can be fine-tuned specifically for OMR","x":1760,"y":-520,"width":360,"height":360,"color":"#ff8f33"},
		{"id":"8a71a1146af2e998","type":"text","text":"\n> 1. **Preprocessing**:  \n>    Standard techniques to ease further steps, e.g., contrast enhancement, binarization, skew-correction, or noise removal. Additionally, the layout should be analyzed to allow subsequent steps to focus on actual content and ignore the background.  \n> \n> 2. **Music Object Detection**:  \n>    Finding and classifying all relevant symbols or glyphs in the image.  \n> \n> 3. **Notation Assembly**:  \n>    Recovering the music notation semantics from the detected and classified symbols. The output is a symbolic representation of the symbols and their relationships, typically as a graph.  \n> \n> 4. **Encoding**:  \n>    Encoding the music into any output format unambiguously, e.g., into MIDI for playback or MusicXML/MEI for further editing in a music notation program.\n>\n>[[understandingOMR.pdf#page=21&selection=8,0,27,64&color=red|understandingOMR, p.21]]\n","x":1653,"y":-1352,"width":595,"height":580,"color":"2"},
		{"id":"77eb74f20dbdc0d6","type":"text","text":"Metrics for transcription ","x":2537,"y":-420,"width":250,"height":60,"color":"3"},
		{"id":"45037687707c6ad4","type":"text","text":"Character error rate (CER), symbol error rate (SER), line error rate (LER)","x":2460,"y":-300,"width":250,"height":180,"color":"3"},
		{"id":"cb42bc70547a93ec","type":"text","text":"E2E OMR with CNN and RNN (monophonic scores)","x":620,"y":-630,"width":250,"height":140,"color":"5"},
		{"id":"ecfb25c2bd1daa98","type":"text","text":"[[End-to-End Neural Optical Music Recognition of Monophonic Scores_compressed.pdf]]\n\npaper proposes a model that combines Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) with a **Connectionist Temporal Classification (CTC)** loss function, enabling training directly from images paired with musical transcripts\n\n[[End-to-end neural OMR of monophonic scores notes]]","x":560,"y":-430,"width":380,"height":360,"color":"5"}
	],
	"edges":[
		{"id":"bc46ceb5ab6ec045","fromNode":"8caf05d5f62eb707","fromSide":"bottom","toNode":"1397383c3bee83ea","toSide":"top","color":"6"},
		{"id":"d3801a0c3a1eea26","fromNode":"edd25bfc0795618e","fromSide":"bottom","toNode":"5eca21d495ec50d5","toSide":"top","color":"2"},
		{"id":"f41aedbaa517b9d1","fromNode":"ddb4bba370f8dbf2","fromSide":"bottom","toNode":"167c0aef7b0a29fc","toSide":"top","color":"2"},
		{"id":"03aa5485f2fbecb6","fromNode":"196af9ae129b7f17","fromSide":"bottom","toNode":"2a5f57fc75eda644","toSide":"top"},
		{"id":"088c597ec4a6d559","fromNode":"c498cbb0891fa771","fromSide":"bottom","toNode":"c10fa58037cf7a00","toSide":"top","color":"3"},
		{"id":"4ac2a24dc3efb85b","fromNode":"aca33838b0d96eed","fromSide":"bottom","toNode":"b2f8988ba798196c","toSide":"top","color":"#aa2bee"},
		{"id":"b7715d12c8f955ef","fromNode":"c498cbb0891fa771","fromSide":"bottom","toNode":"179bb11ce9577fdd","toSide":"top","color":"3"},
		{"id":"fffc5c5d92b94a10","fromNode":"b6e51ec2374d1631","fromSide":"bottom","toNode":"8a71a1146af2e998","toSide":"top","color":"2"},
		{"id":"eca1b8ff1a234a4a","fromNode":"aca33838b0d96eed","fromSide":"right","toNode":"92c216a920bd4678","toSide":"top","color":"#aa2bee"},
		{"id":"09019328eacfbdeb","fromNode":"aca33838b0d96eed","fromSide":"bottom","toNode":"83da196e994781c7","toSide":"top","color":"#aa2bee"},
		{"id":"bb9c6a7dfc868532","fromNode":"cb42bc70547a93ec","fromSide":"bottom","toNode":"ecfb25c2bd1daa98","toSide":"top","color":"5"},
		{"id":"21eb02fd713a83db","fromNode":"e4ca83bfd9458a0e","fromSide":"bottom","toNode":"196af9ae129b7f17","toSide":"top"},
		{"id":"c6c1fc9e735cf0c5","fromNode":"8caf05d5f62eb707","fromSide":"bottom","toNode":"598ed405a08870ce","toSide":"top","color":"6"},
		{"id":"3b6dba1111221fd6","fromNode":"77eb74f20dbdc0d6","fromSide":"bottom","toNode":"45037687707c6ad4","toSide":"top"}
	]
}