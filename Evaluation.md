## Evaluation for project

1. training data with ground truth → compare with correct answer

build confidence for ourselves to see how model performs

we need our own evaluation framework? research on this

2.  manual evaluation
## Current evaluation challenges
### Understanding OMR Paper cites challenges as:
[[Understanding OMR]]
- A review of OMR uses and applications; well-defined in terms of inputs and outputs, and—as much as possible—recommended evaluation methodologies;

The need for a more principled treatment is probably best illustrated by the unsatisfactory state of OMR evaluation. As pointed out by [29], [84], [81], there is still no good way at the moment of how to measure and compare the performance of OMR systems. The lack of such evaluation methods is best illustrated by the way OMR literature presents the state of the field: Some consider it a mature area that works well (at least for typeset music) [61], [62], [5], [12], [135]. Others describe their systems with reports of very high accuracies of up to nearly 100% [147], [99], [163], [91], [104], [110], [122], [162], [33], giving an impression of success; however, many of these numbers are symbol detection scores on a small corpus with a limited vocabulary that are not straightforward to interpret in terms of actual usefulness